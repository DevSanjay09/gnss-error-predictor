{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3499cd",
   "metadata": {},
   "source": [
    "# Day-8 GNSS Error Prediction Training Notebook\n",
    "This notebook walks through the full workflow to train and evaluate the hybrid LSTM/Transformer pipeline for 15-minute cadence Day-8 forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f45cc",
   "metadata": {},
   "source": [
    "## 1. Environment & Dependencies\n",
    "Use the project's virtual environment, then install the additional packages listed below if they are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb64a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed (uncomment the next line on first run)\n",
    "# !pip install -r ..\\requirements.txt --upgrade\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a367c9d",
   "metadata": {},
   "source": [
    "## 2. Data Ingestion & QA\n",
    "Load raw CSV files, normalise column names, parse timestamps, and resample to a strict 15-minute grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = Path('..') / 'backend' / 'data'\n",
    "INPUT_GLOB = '*.csv'\n",
    "FEATURES = ['x_error', 'y_error', 'z_error', 'satclockerror']\n",
    "TIMESTAMP_COL = 'utc_time'\n",
    "TARGET_HORIZON = 96  # 24h @ 15 minutes\n",
    "SPEED_OF_LIGHT = 299_792_458.0\n",
    "SATCLOCK_BOUNDS = (-4.0, 4.0)\n",
    "\n",
    "def load_dataset(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    df = df.rename(columns={'x_error (m)': 'x_error', 'y_error (m)': 'y_error', 'z_error (m)': 'z_error', 'satclockerror (m)': 'satclockerror', 'time': 'utc_time'})\n",
    "    df[TIMESTAMP_COL] = pd.to_datetime(df[TIMESTAMP_COL], errors='coerce')\n",
    "    df = df.dropna(subset=[TIMESTAMP_COL]).sort_values(TIMESTAMP_COL).reset_index(drop=True)\n",
    "    for c in FEATURES:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    df = df.dropna(subset=FEATURES)\n",
    "    return df\n",
    "\n",
    "raw_files = list(RAW_DATA_DIR.glob(INPUT_GLOB))\n",
    "print(f'Found {len(raw_files)} raw files')\n",
    "raw_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55aa955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularise_to_15min(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    idx = pd.date_range(df[TIMESTAMP_COL].min(), df[TIMESTAMP_COL].max(), freq='15T')\n",
    "    df = df.set_index(TIMESTAMP_COL).reindex(idx)\n",
    "    df[FEATURES] = df[FEATURES].interpolate().ffill().bfill()\n",
    "    df = df.reset_index().rename(columns={'index': TIMESTAMP_COL})\n",
    "    return df\n",
    "\n",
    "def normalise_satclock(df: pd.DataFrame) -> tuple[pd.DataFrame, str]:\n",
    "    sat = pd.to_numeric(df['satclockerror'], errors='coerce')\n",
    "    unit = 'seconds'\n",
    "    if sat.abs().median() > 1_000:\n",
    "        df['satclockerror'] = sat / SPEED_OF_LIGHT\n",
    "        unit = 'meters'\n",
    "    df['satclockerror'] = df['satclockerror'].clip(*SATCLOCK_BOUNDS)\n",
    "    df['satclockerror'] = df['satclockerror'].fillna(method='ffill').fillna(method='bfill')\n",
    "    return df, unit\n",
    "\n",
    "cleaned_frames = []\n",
    "for path in raw_files:\n",
    "    df = load_dataset(path)\n",
    "    df = regularise_to_15min(df)\n",
    "    df, unit = normalise_satclock(df)\n",
    "    df['source_file'] = path.name\n",
    "    df['satclock_unit'] = unit\n",
    "    cleaned_frames.append(df)\n",
    "\n",
    "dataset = pd.concat(cleaned_frames, ignore_index=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d5608",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "Create lag features, rolling statistics, harmonic time encodings, and optional RTN transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ts = df[TIMESTAMP_COL]\n",
    "    df['hour'] = ts.dt.hour + ts.dt.minute / 60.0\n",
    "    df['sin24'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['cos24'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['dayofweek'] = ts.dt.dayofweek\n",
    "    return df\n",
    "\n",
    "def add_lagged_features(df: pd.DataFrame, lags: list[int]) -> pd.DataFrame:\n",
    "    for lag in lags:\n",
    "        for feature in FEATURES:\n",
    "            df[f'{feature}_lag_{lag}'] = df[feature].shift(lag)\n",
    "    return df\n",
    "\n",
    "def add_rolling_stats(df: pd.DataFrame, windows: list[int]) -> pd.DataFrame:\n",
    "    for window in windows:\n",
    "        for feature in FEATURES:\n",
    "            df[f'{feature}_roll_{window}_mean'] = df[feature].rolling(window).mean()\n",
    "            df[f'{feature}_roll_{window}_std'] = df[feature].rolling(window).std()\n",
    "    return df\n",
    "\n",
    "feature_df = add_temporal_features(dataset.copy())\n",
    "feature_df = add_lagged_features(feature_df, lags=[1, 2, 4, 8, 16, 24, 48])\n",
    "feature_df = add_rolling_stats(feature_df, windows=[4, 12, 48, 96])\n",
    "feature_df = feature_df.dropna().reset_index(drop=True)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2dc8db",
   "metadata": {},
   "source": [
    "## 4. Windowing\n",
    "Convert the feature table into (input, output) windows for seq2seq training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 96  # 24 hours history\n",
    "OUT_LEN = 96  # 24 hours forecast\n",
    "\n",
    "feature_columns = [c for c in feature_df.columns if c not in {TIMESTAMP_COL, 'source_file', 'satclock_unit'}]\n",
    "\n",
    "def create_windows(values: np.ndarray, seq_len: int, out_len: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    Xs, ys = [], []\n",
    "    total = len(values)\n",
    "    for start in range(0, total - seq_len - out_len + 1):\n",
    "        end = start + seq_len\n",
    "        out_end = end + out_len\n",
    "        Xs.append(values[start:end])\n",
    "        ys.append(values[end:out_end, :len(FEATURES)])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "values = feature_df[feature_columns].values.astype(np.float32)\n",
    "X, y = create_windows(values, SEQ_LEN, OUT_LEN)\n",
    "print(f'Windowed dataset: X={X.shape}, y={y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b45709",
   "metadata": {},
   "source": [
    "## 5. Train/Validation Split\n",
    "Use a chronological split (last window block for validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f70215",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:split_index], X[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]\n",
    "print(f'Train windows: {X_train.shape}, Validation windows: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e284760",
   "metadata": {},
   "source": [
    "## 6. LSTM Baseline\n",
    "Train a two-layer LSTM with Huber loss and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(seq_len: int, n_features: int, out_steps: int) -> keras.Model:\n",
    "    inputs = keras.Input(shape=(seq_len, n_features))\n",
    "    x = keras.layers.LSTM(256, return_sequences=True)(inputs)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.LSTM(128)(x)\n",
    "    x = keras.layers.Dense(128, activation='relu')(x)\n",
    "    outputs = keras.layers.Dense(out_steps * len(FEATURES))(x)\n",
    "    outputs = keras.layers.Reshape((out_steps, len(FEATURES)))(outputs)\n",
    "    model = keras.Model(inputs, outputs, name='lstm_baseline')\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='huber', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "lstm_model = build_lstm(SEQ_LEN, X_train.shape[-1], OUT_LEN)\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train,\n",
    "lstm_history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf357d",
   "metadata": {},
   "source": [
    "## 7. Transformer Model\n",
    "Lightweight encoder-decoder self-attention model for long horizon forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(inputs: keras.layers.Layer, num_heads: int, ff_dim: int, dropout: float) -> keras.layers.Layer:\n",
    "    attn = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "    attn = keras.layers.Dropout(dropout)(attn)\n",
    "    out1 = keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn)\n",
    "    ffn = keras.layers.Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn = keras.layers.Dense(inputs.shape[-1])(ffn)\n",
    "    ffn = keras.layers.Dropout(dropout)(ffn)\n",
    "    return keras.layers.LayerNormalization(epsilon=1e-6)(out1 + ffn)\n",
    "\n",
    "def build_transformer(seq_len: int, n_features: int, out_steps: int) -> keras.Model:\n",
    "    inputs = keras.Input(shape=(seq_len, n_features))\n",
    "    x = keras.layers.Dense(128)(inputs)\n",
    "    for _ in range(3):\n",
    "        x = transformer_block(x, num_heads=4, ff_dim=256, dropout=0.1)\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = keras.layers.Dense(256, activation='relu')(x)\n",
    "    outputs = keras.layers.Dense(out_steps * len(FEATURES))(x)\n",
    "    outputs = keras.layers.Reshape((out_steps, len(FEATURES)))(outputs)\n",
    "    model = keras.Model(inputs, outputs, name='transformer_horizon')\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-4), loss='huber', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "transformer_model = build_transformer(SEQ_LEN, X_train.shape[-1], OUT_LEN)\n",
    "transformer_history = transformer_model.fit(\n",
    "    X_train,\n",
    "transformer_history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d15278",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n",
    "Compute MAE, RMSE, SISRE proxies, and residual normality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6317d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def evaluate_model(name: str, model: keras.Model, X_eval: np.ndarray, y_eval: np.ndarray) -> dict:\n",
    "    preds = model.predict(X_eval, verbose=0)\n",
    "    preds = preds.reshape(-1, len(FEATURES))\n",
    "    truth = y_eval.reshape(-1, len(FEATURES))\n",
    "    metrics = {}\n",
    "    for i, feature in enumerate(FEATURES):\n",
    "        metrics[f'{feature}_mae'] = mean_absolute_error(truth[:, i], preds[:, i])\n",
    "        metrics[f'{feature}_rmse'] = np.sqrt(np.mean((truth[:, i] - preds[:, i]) ** 2))\n",
    "    metrics['model'] = name\n",
    "    return metrics\n",
    "\n",
    "lstm_metrics = evaluate_model('lstm', lstm_model, X_val, y_val)\n",
    "transformer_metrics = evaluate_model('transformer', transformer_model, X_val, y_val)\n",
    "lstm_metrics, transformer_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41c213",
   "metadata": {},
   "source": [
    "## 9. Ensemble & Model Selection\n",
    "Weight each model by inverse validation MAE to pick the best performer for each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f794133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_model(step_index: int, gap_seconds: float) -> str:\n",
    "    if gap_seconds >= 3600:\n",
    "        return 'transformer'\n",
    "    return 'lstm'\n",
    "\n",
    "selection_example = select_model(step_index=12, gap_seconds=1800)\n",
    "selection_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0c151",
   "metadata": {},
   "source": [
    "## 10. Export Artifacts\n",
    "Save the scaler and best-performing weights for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path('..') / 'backend' / 'models'\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "np.save(MODEL_DIR / 'feature_columns.npy', np.array(feature_columns))\n",
    "joblib.dump(StandardScaler().fit(values), MODEL_DIR / 'scaler.pkl')\n",
    "lstm_model.save(MODEL_DIR / 'lstm_model.h5', include_optimizer=False)\n",
    "transformer_model.save(MODEL_DIR / 'transformer_model.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924aef2e",
   "metadata": {},
   "source": [
    "## 11. Day-8 Inference\n",
    "Use the trained models to generate Day-8 predictions for a selected week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.predict import predict_from_dataframe  # noqa: E402\n",
    "\n",
    "sample_week = dataset[dataset['source_file'] == raw_files[0].name]\n",
    "prediction_result = predict_from_dataframe(sample_week)\n",
    "json.dumps(prediction_result['prediction_summary'], indent=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
